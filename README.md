# Enterprise Data Pipeline with Databricks Asset Bundles (DAB), CI/CD & Automated Validation

The project demonstrates end-to-end expertise in ETL pipeline development, data quality validation and CI/CD automation which, deployed using **Databricks Asset Bundles (DAB)** for modern CI/CD practices.

### **Business Requirement:**
Engineered a scalable data pipeline processing **Contact Information** and **Real Estate** datasets through a multi-layer architecture, ensuring data quality, compliance, and production readiness for enterprise analytics.

---

## ğŸš€ Quick Start

### **Project Highlights**
- **Pipeline Architecture**: Bronze â†’ Silver â†’ Gold
- **Automated CI/CD pipeline**: using GithHub Action
- **QA Framework**: Smoke and Regression testing
- **Modern Deployment**: using Databricks Asset Bundles (DAB)
- **Data Quality validation**: performed at every stage of task run

### **CI/CD Deployment**
The GitHub Actions workflow automatically:
1. âœ… Installs correct Databricks CLI
2. âœ… Validates bundle configuration
3. âœ… Deploys to Databricks workspace
4. âœ… Runs validation jobs

---

## ğŸ“‹ Repository File Structure

```
data-pipeline/
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ data_etl.yml            # CI/CD workflow (DAB-based)
â”œâ”€â”€ data_pipeline/              # Core application code
â”‚   â”œâ”€â”€ contact_info/
â”‚   â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ data_generation/
â”‚   â”œâ”€â”€ real_estate/
â”‚   â””â”€â”€ validation/
â”œâ”€â”€ databricks.yml              # DAB configuration (MAIN)
â””â”€â”€ setup.py                    # Python package setup
```

---

## ğŸ—ï¸ End-to-End Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA ENGINEERING PIPELINE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ Data Sources                  ğŸ”„ Processing Layers              ğŸ“Š Analytics
     â”‚                                   â”‚                             â”‚
     â”œâ”€â–º Synthetic Data â”€â”€â”€â”€â”€â”€â”€â”€â–º ğŸŸ¦ Raw Zone â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚
     â”‚   Generator                  â”‚ (Parquet Files)                  â”‚
     â”‚   (Faker)                    â”‚ - No transformation              â”‚
     â”‚                              â”‚ - Batch tracking                 â”‚
     â”‚                              â”‚                                  â”‚
     â”‚                              â–¼                                  â”‚
     â”‚                         ğŸŸ§ Bronze Zone â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚
     â”‚                              â”‚ - Data cleansing                 â”‚
     â”‚                              â”‚ - Null filtering                 â”‚
     â”‚                              â”‚ - Special char removal           â”‚
     â”‚                              â”‚ - Phone standardization          â”‚
     â”‚                              â”‚ - Name normalization             â”‚
     â”‚                              â”‚                                  â”‚
     â”‚                              â–¼                                  â”‚
     â”‚                         ğŸŸ¨ Silver Zone â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚
     â”‚                              â”‚ - Delta Lake tables              â”‚
     â”‚                              â”‚ - Schema evolution               â”‚
     â”‚                              â”‚ - Ready for analytics            â”‚
     â”‚                              â”‚                                  â”‚
     â”‚                              â”‚                                  â”‚
     â”‚                              â–¼                                  â”‚
     â”‚                         ğŸŸ© Gold Zone (Future Planned) â”€â”€â”€â”€â”€â”€â”€â–º â”‚
     â”‚                                - Aggregations                   â”‚
     â”‚                                - Business metrics               â”‚
     â”‚                                - Feature engineering            â”‚
     â”‚                                                                 â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    âœ… Validation Layer (Parallel)
                         â”‚
                         â”œâ”€â–º Smoke Tests (Fast)
                         â”‚   - Business rule validation (YAML-based queries)
                         â”‚   
                         â””â”€â–º Regression Tests (Comprehensive)
                             - Schema validation
                             - Row count checks
                             - Data comparison
```
                             
---

## ğŸ”§ Key End-to-End Pipeline

### **Data Generation Module**
Built synthetic test data for development and testing:
```python
# File: data_pipeline/data_generation/task/generate_data_task.py
def etl_process(**options):
    """Generate realistic synthetic data using Faker"""
    fake = Faker()
    
    # Intelligent batch ID management
    batch_id = batch_ids_processing(path)  # Auto-increments from last batch
    
    # Generate records with realistic patterns
    for i in range(num_rows):
        data.append({
            "profile_id": fake.uuid4(),
            "first_name": random_cases(fake.first_name()),
            "phone_personal": fake.phone_number(),
            # ... 20+ fields with realistic data
        })
```
Key features:
- âœ… Automatic batch versioning
- âœ… Realistic data patterns with intentional errors
- âœ… Data quality issues for testing (null, special chars, etc.)
- âœ… Separating datasets for Contact Info and Real Estate
- âœ… Store all datasets into a parquet files

### **Raw Layer Extractions**
```python
# File: data_pipeline\real_estate\task\raw\re_extract_data_task.py
def etl_process(**options):
    # Fetch all the files in the folder
    based_path = "/Volumes/data_lake_dev/feature_raw_data/real_estate_parquet/"
    files = dbutils.fs.ls(based_path)

    # Table existence check and append the table
    spark.sql(f"CREATE TABLE IF NOT EXISTS {re_raw_loc} USING DELTA")
    re_df.write.format("delta").mode("append").option("mergeSchema", "true").saveAsTable(re_raw_loc)
```
Reasons raw layer is lean:
- âœ… Faster downstream iteration (as Bronze handle heavy processes)
- âœ… Simplifies reprocessing (rerun Bronze/Silver without re-extract)

### **Bronze Layer Transformation**
```python
# File: data_pipeline\contact_info\task\bronze\ci_transform_data_task.py
def etl_process(**options):
    """Data Quality Transformations"""
    
    # Filter NULL values in critical fields
    filter_null_df = ci_raw.filter(
        ~F.expr("first_name IS NULL AND last_name IS NULL")
    )
    
    # Remove special characters using regex
    spec_char_rmv_df = filter_null_df.withColumn(
        "first_name", 
        F.regexp_replace("first_name", r"(?i)[^a-z0-9_-]", "")
    )
    
    # Standardize phone numbers to US format
    format_us_phone_udf = F.udf(us_format_phone, StringType())
    std_phone_df = spec_char_rmv_df.withColumn(
        "std_phone", format_us_phone_udf(F.col("phone"))
    )
    
    # Name normalization (lowercase, concatenation)
    std_name = name_df.withColumn(
        "std_full_name", 
        F.lower(F.concat_ws(" ", "first_name", "middle_name", "last_name"))
    )
```
Transformations applied:
- âœ… NULL handling
- âœ… Regex-based special characters removal
- âœ… Phone number standardization
- âœ… Name standardization

### **Silver Layer Loading**
```python
# File: data_pipeline\contact_info\task\silver\ci_load_data_task.py
def etl_process(**options):
    """Load to Silver with Delta Lake features"""
    
    # Enable schema evolution for flexibility
    ci_bronze.write.format("delta")\
        .mode("append")\
        .option("mergeSchema", "true")\
        .saveAsTable(ci_silver_loc)
```
Delta lake benefits:
- âœ… Data versioning and time travel capabilities
- âœ… Data reliability and consistency (prevent data corruption & allow rollbacks)

### **QA Validation Framework**
Configuration-Driven Testing:
```python
# File: data_pipeline\core\validation\config\smoke\qa_config_cip_smoke.csv
environment,space,object_type,zone,job_type,test_type,check_type,assert_type
dev,synthetic,ci,raw,cip,smoke,software,hard_stop
dev,synthetic,ci,raw,cip,smoke,data_quality,soft_stop
dev,synthetic,ci,silver,cip,smoke,software,hard_stop
```

Validation Framework Architecture:
```python
# File: data_pipeline\validation\task\rep_val.py
def etl_process(**options):
    # Smoke Test run 
    if test_type == 'smoke': 
        print("Running smoke tests...")
        smoke_run_validation(use_case_id, config, bucket, catalog, env, space, job_type, smoke_object_type, smoke_zone, test_type, check_types, batch_id, flow, property_schema) 

    # Regression Test run 
    elif test_type == 'regression':   
        print("Running regression tests...")
```
Key features:
- âœ… Streamlined CSV-driven test configuration
- âœ… Hard/soft assertion modes
- âœ… Automated test report generation

---
